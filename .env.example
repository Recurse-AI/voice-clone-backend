# ===========================================
# WHISPERX CONFIGURATION
# ===========================================

# WhisperX Model Size - Options: tiny, base, small, medium, large-v2, large-v3
# Memory Requirements:
# large-v2: ~3GB VRAM, Best accuracy (recommended for >16GB GPU)
# medium: ~1.5GB VRAM, Good balance (recommended for 8-16GB GPU)
# small: ~1GB VRAM, Faster, decent accuracy (recommended for 6-10GB GPU)
# base: ~500MB VRAM, Even faster, basic accuracy
# tiny: ~200MB VRAM, Fastest, minimal accuracy
# 
# For RunPod with Fish Speech: Use 'small' or 'base' to avoid CUDA OOM
WHISPER_MODEL_SIZE=small

# Compute Type - Options: auto, float16, int8
# auto: Automatically selects best option (float16 for GPU, int8 for CPU)
# float16: GPU optimized, high quality
# int8: CPU optimized, good quality with lower memory usage
WHISPER_COMPUTE_TYPE=auto

# Languages to preload alignment models for (comma-separated)
# IMPORTANT: Reduce list for lower GPU memory usage!
# Preloading reduces first-time transcription latency for these languages
# Common language codes: en, es, fr, de, it, pt, ru, ja, ko, zh, ar, hi, bn
# For 8-16GB GPU: Use only 5-7 languages to avoid CUDA OOM
WHISPER_PRELOAD_LANGUAGES=en,es,fr,de,it

# GPU Memory Optimization
# Set to True to avoid memory fragmentation issues
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# ===========================================
# FISH SPEECH MEMORY OPTIMIZATION
# ===========================================

# Fish Speech Low Memory Mode (recommended for <16GB GPU)
# true: Use aggressive memory optimization (saves ~2-4GB VRAM)
# false: Use normal mode (better performance, more VRAM)
FISH_SPEECH_LOW_MEMORY=true

# Fish Speech Model Compilation (advanced users only)
# true: Compile model for faster inference (uses more memory initially)
# false: No compilation (saves memory, slightly slower)
FISH_SPEECH_COMPILE=false

# ===========================================
# OTHER CONFIGURATION (add your existing vars here)
# ===========================================