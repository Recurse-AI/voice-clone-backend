import os
import json
import logging
import time
from app.config.settings import settings
from .assembly_transcription import TranscriptionService
from .fish_speech_service import get_fish_speech_service
from app.services.r2_service import get_r2_service
from .manifest_service import (
    build_manifest,
    save_manifest_to_dir,
    upload_process_dir_to_r2,
    enrich_and_reupload_manifest_with_urls,
)
import numpy as np
from .audio_utils import AudioUtils
from typing import Optional, Dict, Any
from app.utils.unified_status_manager import get_unified_status_manager, ProcessingStatus, JobType
import threading
import re

logger = logging.getLogger(__name__)

_simple_dubbed_api_instance = None
_api_lock = threading.Lock()
def smart_chunk(text: str, chunk_size: int = 200, min_size: int = 180) -> list[str]:
    if not text:
        return []

    text = text.strip()
    if len(text) <= chunk_size:
        return [text]

    sentence_break_re = re.compile(r"(?<=[\.\!\?„ÄÇÔºÅÔºüÿüÿõ‚Ä¶])")
    sentences = sentence_break_re.split(text)

    chunks: list[str] = []
    current = ""

    for sent in sentences:
        if not sent:
            continue
        if len(current) + len(sent) <= chunk_size or len(current) < min_size:
            current += sent
        else:
            chunks.append(current.strip())
            current = sent
    if current.strip():
        chunks.append(current.strip())

    final_chunks: list[str] = []
    for ch in chunks:
        if len(ch) <= chunk_size:
            final_chunks.append(ch)
            continue

        words = ch.split()
        curr_chunk = words[0]
        for word in words[1:]:
            if len(curr_chunk) + 1 + len(word) > chunk_size:
                final_chunks.append(curr_chunk)
                curr_chunk = word
            else:
                curr_chunk += " " + word
        if curr_chunk:
            final_chunks.append(curr_chunk)

    return [c for c in final_chunks if c]

from app.services.language_service import language_service

class SimpleDubbedAPI:
    def __init__(self):
        self.transcription_service = TranscriptionService()
        self.fish_speech = get_fish_speech_service()
        self._r2_storage = None
    
    @property
    def r2_storage(self):
        if self._r2_storage is None:
            self._r2_storage = get_r2_service()
        return self._r2_storage
    
    @property  
    def temp_dir(self):
        return settings.TEMP_DIR
    
    def _update_status_non_blocking(self, job_id: str, status: ProcessingStatus, progress: int, details: dict, job_type: str = "dub"):
        import asyncio
        manager = get_unified_status_manager()
        job_type_enum = JobType.DUB if job_type == "dub" else JobType.SEPARATION
        
        # Run async update in thread pool for non-blocking execution
        def run_update():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(
                    manager.update_status(job_id, job_type_enum, status, progress, details)
                )
            finally:
                loop.close()
        
        import threading
        threading.Thread(target=run_update, daemon=True).start()
    
    def _check_cancellation(self, job_id: str) -> bool:
        from app.utils.shared_memory import is_job_cancelled
        if is_job_cancelled(job_id):
            self._update_status_non_blocking(job_id, ProcessingStatus.CANCELLED, 0, {
                "message": "Job cancelled by user", "error": "Job cancelled by user"
            }, "dub")
            return True
        return False
    
    def dub_text_batch(self, segments: list, target_language: str = "English", batch_size: int = 10, job_id: str = None) -> list:
        all_dubbed = []
        for i in range(0, len(segments), batch_size):
            if job_id and self._check_cancellation(job_id):
                return []
                
            batch = segments[i:i+batch_size]
            prompt_lines = []
            for idx, seg in enumerate(batch):
                prompt_lines.append(f"[{idx+1}] (Target duration: {seg['duration_ms']} ms) {seg['text']}")
            joined_texts = "\n".join(prompt_lines)
            
            system_prompt = (
                f"You are assisting in creating dubbing scripts for the Fish Audio OpenAudio-S1 TTS model.\n"
                f"Translate each input segment into {target_language} (keeping meaning accurate).\n"
                f"Constraints for every translated segment:\n"
                f"1. Try to match the target duration (given in ms) ‚Äî if you need to lengthen, prefer inserting extra *spaces* between words rather than adding new words.\n"
                f"2. Use the correct alphabet/script for {target_language}; never mix English letters unless the original word is a proper noun or acronym.\n"
                f"3. You MAY optionally use the Fish-Audio emotion/tone markers like (excited), (sad), (whispering) etc. **only** when that better reflects the original intent. Place the marker at the very beginning of the sentence.\n"
                f"4. Do NOT add any explanatory text, numbering, or comments ‚Äî only the final translated sentences.\n"
                f"5. Return the translated segments in the same order, separated by ||| on a single line."
            )
            
            user_prompt = (
                "Translate each segment below. Return only the translated segments, in order, separated by |||.\n"
                "Segments (with target duration):\n"
                f"{joined_texts}"
            )
            try:
                response = self.transcription_service.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.3,
                    max_tokens=2048
                )
                output = response.choices[0].message.content.strip()
                all_dubbed.extend([seg.strip() for seg in output.split("|||")])
            except ConnectionResetError as e:
                logger.warning(f"Connection reset error during OpenAI call, retrying batch {i//batch_size + 1}: {e}")
                # Retry once with a shorter delay
                try:
                    time.sleep(1)
                    response = self.transcription_service.openai_client.chat.completions.create(
                        model="gpt-4o",
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        temperature=0.3,
                        max_tokens=2048
                    )
                    output = response.choices[0].message.content.strip()
                    all_dubbed.extend([seg.strip() for seg in output.split("|||")])
                except Exception as retry_error:
                    logger.error(f"Failed to process batch {i//batch_size + 1} after retry: {retry_error}")
                    # Add placeholder translations to maintain segment count
                    all_dubbed.extend([f"[Translation Error for segment {j+1}]" for j in range(len(batch))])
            except Exception as e:
                logger.error(f"Error processing batch {i//batch_size + 1}: {e}")
                # Add placeholder translations to maintain segment count
                all_dubbed.extend([f"[Translation Error for segment {j+1}]" for j in range(len(batch))])
        return all_dubbed

    def _setup_processing_directory(self, job_id: str, output_dir: str = None) -> str:
        """Setup and ensure processing directory exists."""
        if not output_dir:
            output_dir = os.path.join(self.temp_dir, f"dub_{job_id}")
        os.makedirs(output_dir, exist_ok=True)
        return output_dir

    def process_dubbed_audio(self, audio_url: str, job_id: str, target_language: str, 
                           speakers_count: int = 1, source_video_language: str = None, 
                           output_dir: str = None, review_mode: bool = False, 
                           manifest_override: Optional[Dict[str, Any]] = None) -> dict:
        """
        Complete dubbed audio processing with clean, modular approach.
        Always generates SRT file. No audio mixing or conditional processing.
        """
        # 1. Validate and normalize language
        if not language_service.is_dubbing_supported(target_language):
            supported_langs = language_service.get_supported_dubbing_languages()
            error_msg = f"Unsupported target language: {target_language}. Supported languages: {', '.join(sorted(supported_langs))}"
            logger.error(error_msg)
            return {"success": False, "error": error_msg}
        
        target_language_code = language_service.normalize_language_input(target_language)
        logger.info(f"Processing with target language: {target_language} -> {target_language_code}")

        try:
            # 2. Setup processing environment
            process_temp_dir = self._setup_processing_directory(job_id, output_dir)
            
            # üõ°Ô∏è Check cancellation before transcription
            if self._check_cancellation(job_id):
                return {"success": False, "error": "Job cancelled by user"}
            
            # Get transcription data
            raw_sentences, transcript_id = self._get_transcription_data(
                job_id, audio_url, manifest_override, process_temp_dir, speakers_count, source_video_language
            )
            
            # üõ°Ô∏è Check cancellation before dubbing and voice cloning
            if self._check_cancellation(job_id):
                return {"success": False, "error": "Job cancelled by user"}
            
            # Process text dubbing and voice cloning
            dubbed_segments = self._process_dubbing_and_cloning(
                job_id, raw_sentences, target_language, manifest_override, review_mode, process_temp_dir
            )
            
            if review_mode:
                # Preparing review files - update to 80%
                self._update_status_non_blocking(
                    job_id,
                    ProcessingStatus.PROCESSING,
                    80,
                    {"message": "Preparing review files"},
                    "dub",
                )
                logger.info("Preparing review files for human review...")
                # Build and save manifest
                manifest = build_manifest(job_id, transcript_id, target_language, dubbed_segments)
                manifest_path = save_manifest_to_dir(manifest, process_temp_dir, job_id)
                folder_upload_result, manifest_url, manifest_key = upload_process_dir_to_r2(job_id, process_temp_dir, self.r2_storage)

                # best-effort enrich and re-upload
                try:
                    files_map = {}
                    if isinstance(folder_upload_result, dict):
                        for fname, res in folder_upload_result.items():
                            if res.get("success"):
                                files_map[fname] = {"url": res.get("url"), "r2_key": res.get("r2_key")}
                    enriched_url = enrich_and_reupload_manifest_with_urls(manifest, manifest_path, files_map, manifest_key)
                    if enriched_url:
                        manifest_url = enriched_url
                except Exception:
                    pass

                # Ensure manifest_url is available before returning
                if not manifest_url:
                    logger.error(f"Failed to get manifest URL for review mode job {job_id}")
                    return {"success": False, "error": "Failed to generate manifest URL for review"}

                # Don't remove temp directory in review mode - needed for segments access
                logger.info(f"Review mode: preserving temp directory for segments access: {process_temp_dir}")
                return {
                    "success": True,
                    "job_id": job_id,
                    "review": {
                        "segments_manifest_url": manifest_url,
                        "segments_manifest_key": manifest_key,
                        "segments_count": len(dubbed_segments),
                        "transcript_id": transcript_id
                    },
                    "folder_upload": folder_upload_result
                }

            # üõ°Ô∏è Check cancellation before final output generation
            if self._check_cancellation(job_id):
                return {"success": False, "error": "Job cancelled by user"}
            
            # Generate final audio and files
            return self._generate_final_output(
                job_id, dubbed_segments, process_temp_dir, 
                target_language, audio_url, speakers_count, transcript_id
            )
        except Exception as e:
            logger.error(f"Dubbed processing failed: {str(e)}")
            # Clean up temp directory on exception
            if locals().get("process_temp_dir"):
                AudioUtils.remove_temp_dir(folder_path=locals().get("process_temp_dir"))
            return {"success": False, "error": str(e)}
    
    def _voice_clone_segment(self, dubbed_text: str, reference_audio_path: str, segment_id: str, original_text: str = "", speaker_label: str = None, job_id: str = None, process_temp_dir: str = None) -> Optional[Dict[str, Any]]:
        """Voice clone dubbed text using FishSpeechService with reference audio and transcript_id in filename"""
        try:
            if not reference_audio_path:
                logger.warning(f"No reference audio path provided for {segment_id}")
                return None
            if not os.path.exists(reference_audio_path):
                logger.warning(f"Reference audio file not found: {reference_audio_path} for {segment_id}")
                return None
            logger.info(f"Voice cloning {segment_id} using reference: {reference_audio_path}")
            import soundfile as sf
            import io
            audio_data, sample_rate = sf.read(reference_audio_path)
            # Convert to mono if stereo
            if len(audio_data.shape) > 1:
                audio_data = audio_data[:, 0]

            # Limit reference audio length based on configuration
            from app.config.settings import settings
            max_ref_seconds = settings.MAX_REFERENCE_SECONDS
            total_seconds = len(audio_data) / sample_rate
            if total_seconds > max_ref_seconds:
                # take the beginning slice of length max_ref_seconds for better alignment
                end_sample = int(max_ref_seconds * sample_rate)
                audio_data = audio_data[:end_sample]

            buffer = io.BytesIO()
            sf.write(buffer, audio_data, sample_rate, format='WAV')
            reference_audio_bytes = buffer.getvalue()
            if not reference_audio_bytes:
                logger.error(f"Failed to load reference audio: {reference_audio_path}")
                return None
            # Extract segment index from segment_id (seg_001 -> 0)
            segment_index = int(segment_id.split('_')[1]) - 1
            cloned_filename = f"cloned_{job_id}_{segment_index:03d}.wav"
            cloned_path = os.path.join(process_temp_dir, cloned_filename).replace('\\', '/')
            # Split dubbed text into manageable chunks
            text_chunks = smart_chunk(dubbed_text, chunk_size=200, min_size=180)
            audio_chunks = []
            sample_rate_out = None
            seed_val = None
            if speaker_label:
                seed_val = abs(hash(speaker_label)) % (2**32)
            for chunk in text_chunks:
                result = self.fish_speech.generate_with_reference_audio(
                    text=chunk,
                    reference_audio_bytes=reference_audio_bytes,
                    reference_text=original_text or "Reference audio",
                    max_new_tokens=2048,
                    top_p=0.7,
                    repetition_penalty=1.2,
                    temperature=0.7,
                    seed=seed_val,
                    chunk_length=200
                )
                if result.get("success"):
                    import soundfile as sf
                    import io
                    buffer = io.BytesIO(result["audio_data"])
                    audio, sample_rate = sf.read(buffer)
                    if len(audio.shape) > 1:
                        audio = audio[:, 0]
                    audio_chunks.append(audio)
                    sample_rate_out = sample_rate
            if audio_chunks:
                final_audio = np.concatenate(audio_chunks)
                import soundfile as sf
                buffer = io.BytesIO()
                sf.write(buffer, final_audio, sample_rate_out, format='WAV')
                with open(cloned_path, "wb") as f:
                    f.write(buffer.getvalue())
                duration_ms = int(len(final_audio) / sample_rate_out * 1000)
                return {"path": cloned_path, "duration_ms": duration_ms}
            return None
                
        except Exception as e:
            logger.error(f"Voice cloning error for {segment_id}: {str(e)}")
            return None
    
    def _get_transcription_data(self, job_id: str, audio_url: str, manifest_override: Optional[Dict[str, Any]], 
                               process_temp_dir: str, speakers_count: int, source_video_language: str) -> tuple:
        """Get transcription data either from manifest override or by transcribing audio"""
        self._update_status_non_blocking(job_id, ProcessingStatus.PROCESSING, 45, {"message": "Transcribing audio..."}, "dub")
        logger.info("Starting transcription...")
        
        transcript_id = None
        raw_sentences = []
        
        if manifest_override:
            logger.info("Using manifest override data instead of re-transcribing")
            for idx, seg in enumerate(manifest_override.get("segments", [])):
                raw_sentences.append({
                    "text": seg.get("original_text", ""),
                    "start": seg.get("start", 0),
                    "end": seg.get("end", 0),
                    "speaker_label": seg.get("speaker_label"),
                    "output_path": os.path.join(process_temp_dir, seg.get("original_audio_file", f"segment_{idx:03d}.wav")).replace('\\', '/')
                })
            transcript_id = manifest_override.get("transcript_id")
            
            # Download original segment audios to temp dir if URLs available
            self._download_segment_audios(manifest_override, process_temp_dir)
            
        else:
            logger.info("Starting fresh transcription")
            sentences_result = self.transcription_service.get_sentences_and_split_audio(
                audio_url=audio_url,
                output_dir=process_temp_dir,
                speakers_count=speakers_count,
                source_video_language=source_video_language,
                job_id=job_id
            )
            
            if not sentences_result["success"]:
                logger.error(f"Transcription failed: {sentences_result.get('error')}")
                AudioUtils.remove_temp_dir(folder_path=process_temp_dir)
                raise Exception(sentences_result.get("error", "Transcription failed"))
                
            raw_sentences = sentences_result["segments"]
            transcript_id = sentences_result.get("transcript_id")
            logger.info(f"Found {len(raw_sentences)} sentence segments")
        
        return raw_sentences, transcript_id
    
    def _process_dubbing_and_cloning(self, job_id: str, raw_sentences: list, target_language: str,
                                     manifest_override: Optional[Dict[str, Any]], review_mode: bool, 
                                     process_temp_dir: str) -> list:
        """Process text dubbing and voice cloning for all segments"""
        self._update_status_non_blocking(job_id, ProcessingStatus.PROCESSING, 55, {"message": "Batch dubbing text..."}, "dub")
        logger.info("Starting batch dubbing...")
        
        # Prepare texts for batch dubbing
        segments_to_dub = []
        segment_indices = []
        for i, segment in enumerate(raw_sentences):
            original_text = segment.get("text", "")
            start_ms = segment.get("start", 0)
            end_ms = segment.get("end", 0)
            if original_text.strip():
                segments_to_dub.append({
                    "text": original_text,
                    "duration_ms": end_ms - start_ms,
                    "id": f"seg_{i+1:03d}"
                })
                segment_indices.append(i)
        
        # Normalize target language for consistent processing
        target_language_code = language_service.normalize_language_input(target_language)
        dubbed_texts = self.dub_text_batch(segments_to_dub, target_language_code, batch_size=15, job_id=job_id)
        
        # OpenAI dubbing completed - update to 75%
        self._update_status_non_blocking(
            job_id,
            ProcessingStatus.PROCESSING,
            75,
            {"message": "Reviewing and editing with AI"},
            "dub",
        )
        logger.info("OpenAI dubbing completed, starting voice processing...")
        
        # Apply edited text overrides if present (only for same target language)
        edited_map = {}
        if manifest_override:
            # Check if this is a redub with language change
            is_redub = bool(manifest_override.get("redub_target_language"))
            manifest_target_lang = manifest_override.get("target_language")
            current_target_lang = language_service.normalize_language_input(target_language)
            
            if is_redub:
                # For redub: compare original manifest language with current target
                if manifest_target_lang:
                    manifest_target_lang = language_service.normalize_language_input(manifest_target_lang)
                    
                    # Only apply overrides if it's same language redub (corrections)
                    if manifest_target_lang == current_target_lang:
                        for seg in manifest_override.get("segments", []):
                            if seg.get("id") and seg.get("dubbed_text"):
                                edited_map[seg["id"]] = seg["dubbed_text"]
            else:
                # Fresh job or continuation: always apply any edited text
                for seg in manifest_override.get("segments", []):
                    if seg.get("id") and seg.get("dubbed_text"):
                        edited_map[seg["id"]] = seg["dubbed_text"]
        
        # Start voice cloning phase at 80% (after review preparation)
        self._update_status_non_blocking(
            job_id,
            ProcessingStatus.PROCESSING,
            80,
            {"message": "Voice cloning and reconstructing final audio"},
            "dub",
        )
        logger.info("Starting voice cloning...")
        
        # Process voice cloning for each segment
        dubbed_segments = []
        dub_idx = 0

        # Calculate dynamic progress range for voice cloning updates (80..89)
        cloneable_count = sum(1 for s in raw_sentences if s.get("text", "").strip()) if not review_mode else 0
        completed_clones = 0
        last_progress_update = 80  # Track last progress to avoid frequent updates

        for i, segment in enumerate(raw_sentences):
            # üõ°Ô∏è Check cancellation in voice cloning loop
            if self._check_cancellation(job_id):
                return []  # Return empty list to stop processing
            
            seg_id = f"seg_{i+1:03d}"
            start_ms = segment.get("start", 0)
            end_ms = segment.get("end", 0)
            original_text = segment.get("text", "")
            speaker_label = segment.get("speaker_label")
            original_audio_path = segment.get("output_path", "")
            
            if not original_text.strip():
                continue
                
            dubbed_text = dubbed_texts[dub_idx]
            dub_idx += 1
            
            # Apply edited text override if present
            if seg_id in edited_map:
                dubbed_text = edited_map[seg_id]
            
            # Voice clone if not in review mode
            cloned_audio_path = None
            cloned_duration_ms = end_ms - start_ms
            if not review_mode:
                clone_result = self._voice_clone_segment(
                    dubbed_text, original_audio_path, seg_id, original_text, 
                    speaker_label, job_id=job_id, process_temp_dir=process_temp_dir
                )
                if clone_result:
                    cloned_audio_path = clone_result.get("path")
                    cloned_duration_ms = clone_result.get("duration_ms", end_ms - start_ms)
                
                # Update progress only at significant intervals (every 10% of cloning or every 5 segments)
                try:
                    if cloneable_count > 0:
                        completed_clones += 1
                        # Calculate progress but only update at meaningful intervals
                        progress_value = 80 + min(9, int((completed_clones / max(1, cloneable_count)) * 9))
                        
                        # Only update if progress increased by at least 2% or every 5 segments
                        if (progress_value - last_progress_update >= 2 or 
                            completed_clones % 5 == 0 or 
                            completed_clones == cloneable_count):
                            
                            self._update_status_non_blocking(
                                job_id,
                                ProcessingStatus.PROCESSING,
                                progress_value,
                                {
                                    "message": "Voice cloning and reconstructing final audio",
                                    "cloned_segments": completed_clones,
                                    "total_segments": cloneable_count,
                                },
                                "dub",
                            )
                            last_progress_update = progress_value
                except Exception:
                    # Best-effort progress update; ignore errors
                    pass
            
            # Create segment info and save to JSON
            segment_json = self._create_segment_info(
                seg_id, i, start_ms, cloned_duration_ms, original_text, dubbed_text,
                original_audio_path, cloned_audio_path, speaker_label, job_id, process_temp_dir
            )
            dubbed_segments.append(segment_json)
        
        return dubbed_segments
    
    def _create_segment_info(self, seg_id: str, segment_index: int, start_ms: int, cloned_duration_ms: int,
                           original_text: str, dubbed_text: str, original_audio_path: str, 
                           cloned_audio_path: str, speaker_label: str, job_id: str, process_temp_dir: str) -> dict:
        """Create and save segment information JSON"""
        info_filename = f"segment_{job_id}_{segment_index:03d}_info.json"
        info_path = os.path.join(process_temp_dir, info_filename).replace('\\', '/')
        
        segment_json = {
            "id": seg_id,
            "segment_index": segment_index + 1,
            "start": start_ms,
            "end": start_ms + cloned_duration_ms,
            "duration_ms": cloned_duration_ms,
            "original_text": original_text,
            "dubbed_text": dubbed_text,
            "original_audio_file": f"segment_{segment_index:03d}.wav" if original_audio_path else None,
            "cloned_audio_file": f"cloned_{job_id}_{segment_index:03d}.wav" if cloned_audio_path else None,
            "voice_cloned": bool(cloned_audio_path),
            "original_audio_path": original_audio_path,
            "cloned_audio_path": cloned_audio_path,
            "speaker_label": speaker_label,
        }
        
        try:
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(segment_json, f, ensure_ascii=False, indent=2)
            logger.info(f"Saved segment info: {info_filename}")
        except Exception as e:
            logger.error(f"Failed to save JSON for {seg_id}: {str(e)}")
        
        return segment_json

    def _download_segment_audios(self, manifest_override: Dict[str, Any], process_temp_dir: str) -> None:
        """Download original segment audios from manifest URLs"""
        for seg in manifest_override.get("segments", []):
            url = seg.get("original_audio_url")
            fname = seg.get("original_audio_file")
            if url and fname:
                try:
                    import requests
                    resp = requests.get(url, timeout=60)
                    resp.raise_for_status()
                    with open(os.path.join(process_temp_dir, fname), 'wb') as fw:
                        fw.write(resp.content)
                    logger.info(f"Downloaded segment audio: {fname}")
                except Exception as e:
                    logger.warning(f"Failed to download original segment {fname}: {e}")

    def _generate_final_output(self, job_id: str, dubbed_segments: list, process_temp_dir: str, 
                              target_language: str, audio_url: str, speakers_count: int, transcript_id: str) -> dict:
        """Generate final audio, SRT file, and upload to R2"""
        self._update_status_non_blocking(job_id, ProcessingStatus.PROCESSING, 90, {"message": "Reconstructing final audio..."}, "dub")
        logger.info("Reconstructing final audio...")
        
        # Reconstruct final audio
        final_audio_path = self._reconstruct_final_audio(dubbed_segments, None, job_id=job_id, process_temp_dir=process_temp_dir)
        
        # Generate SRT file and finalize (combining multiple close steps)
        self._update_status_non_blocking(job_id, ProcessingStatus.PROCESSING, 92, {"message": "Finalizing output files..."}, "dub")
        
        subtitle_path = self._generate_srt_file(job_id, dubbed_segments, process_temp_dir)
        
        # Create process summary
        self._create_process_summary(job_id, dubbed_segments, final_audio_path, subtitle_path, 
                                   process_temp_dir, target_language, audio_url, speakers_count, transcript_id)
        
        # Upload to R2 and get results
        return self._upload_and_finalize(job_id, process_temp_dir, final_audio_path)
    
    def _generate_srt_file(self, job_id: str, dubbed_segments: list, process_temp_dir: str) -> str:
        """Generate SRT subtitle file"""
        logger.info("Generating SRT file...")
        
        from .video_processor import VideoProcessor
        processor = VideoProcessor(temp_dir=process_temp_dir)
        subtitle_data = []
        
        for seg in dubbed_segments:
            text = seg["dubbed_text"]
            start = seg["start"] / 1000.0
            end = seg["end"] / 1000.0

            # Use smart_chunk utility to create subtitle lines
            chunks = smart_chunk(text, chunk_size=60, min_size=40)
            total_chars = sum(len(c) for c in chunks) or 1

            char_count_before = 0
            for chunk in chunks:
                chunk_len = len(chunk)
                chunk_start = start + (end - start) * (char_count_before / total_chars)
                char_count_before += chunk_len
                chunk_end = start + (end - start) * (char_count_before / total_chars)
                subtitle_data.append({"start": chunk_start, "end": min(chunk_end, end), "text": chunk})
        
        subtitle_path = os.path.join(process_temp_dir, f"subtitles_{job_id}.srt")
        processor.create_srt_file(subtitle_data, subtitle_path)
        logger.info(f"Subtitle file saved: {subtitle_path}")
        return subtitle_path
    
    def _create_process_summary(self, job_id: str, dubbed_segments: list, final_audio_path: str, 
                               subtitle_path: str, process_temp_dir: str, target_language: str, 
                               audio_url: str, speakers_count: int, transcript_id: str) -> None:
        """Create and save process summary JSON"""
        # Check for instrument and vocal files
        instrument_file = f"instruments_{job_id}.wav"
        vocal_file = f"vocals_{job_id}.wav"
        
        if not os.path.exists(os.path.join(process_temp_dir, instrument_file)):
            instrument_file = None
            
        if not os.path.exists(os.path.join(process_temp_dir, vocal_file)):
            vocal_file = None
        
        process_summary = {
            "success": True,
            "job_id": job_id,
            "segments_count": len(dubbed_segments),
            "audio_url": audio_url,
            "target_language": target_language,
            "speakers_count": speakers_count,
            "final_audio_file": os.path.basename(final_audio_path) if final_audio_path else None,
            "subtitle_file": os.path.basename(subtitle_path) if subtitle_path else None,
            "instrument_file": instrument_file,
            "vocal_file": vocal_file,
            "final_video_file": None,  # Video creation disabled
            "processing_timestamp": int(time.time()),
            "segments": dubbed_segments,
            "transcript_id": transcript_id
        }
        
        summary_filename = f"dubbing_process_summary_{job_id}.json"
        summary_path = os.path.join(process_temp_dir, summary_filename).replace('\\', '/')
        
        try:
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(process_summary, f, ensure_ascii=False, indent=2)
            logger.info(f"Process summary saved: {summary_filename}")
        except Exception as e:
            logger.error(f"Failed to save process summary: {str(e)}")
    
    def _upload_and_finalize(self, job_id: str, process_temp_dir: str, final_audio_path: str) -> dict:
        """Upload files to R2 and return final results"""
        self._update_status_non_blocking(job_id, ProcessingStatus.PROCESSING, 95, {"message": "Uploading and finalizing..."}, "dub")
        
        # Upload entire directory to R2
        folder_upload_result = self.r2_storage.upload_directory(job_id, process_temp_dir)
        
        # Note: Vocal/instrument files use runpod URLs directly, no R2 upload needed
        
        # All files (including vocals/instruments) are available via folder_upload_result
        
        logger.info("Dubbed processing completed successfully")
        
        # Do not remove temp dir here; caller handles cleanup after final status update
        
        return {
            "success": True,
            "job_id": job_id,
            "result_url": None,  # No video result
            "result_urls": {},
            "folder_upload": folder_upload_result,
            "video_upload": None,
            "video_error": None
        }
    



    def _reconstruct_final_audio(self, segments: list, original_audio_path: str, job_id: str = None, process_temp_dir: str = None) -> str:
        """Reconstruct final audio with exact original duration.
        Missing segments will have silent audio automatically.
        """
        try:
            import soundfile as sf
            if not segments:
                return None
                
            sample_rate = None
            base_dtype = np.float32
            original_duration_samples = 0
            
            # Get original audio properties for exact duration matching
            if original_audio_path and os.path.exists(original_audio_path):
                original_audio, sample_rate = sf.read(original_audio_path)
                if len(original_audio.shape) > 1:
                    original_audio = original_audio[:, 0]
                base_dtype = original_audio.dtype if hasattr(original_audio, 'dtype') else np.float32
                original_duration_samples = len(original_audio)
            else:
                # Fallback: derive sample rate from cloned segments
                for segment in segments:
                    cloned_path = segment.get("cloned_audio_path")
                    if cloned_path and os.path.exists(cloned_path):
                        cloned_audio, sample_rate = sf.read(cloned_path)
                        base_dtype = cloned_audio.dtype if hasattr(cloned_audio, 'dtype') else np.float32
                        break
                if not sample_rate:
                    sample_rate = 44100
                # Calculate duration from segments if no original audio
                segment_end_times = [s.get("end", 0) for s in segments if s.get("end")]
                if segment_end_times:
                    max_end_time_ms = max(segment_end_times)
                    original_duration_samples = int((max_end_time_ms / 1000.0) * sample_rate)
                else:
                    # Fallback to 0 if no valid segments
                    original_duration_samples = 0
            
            # Create final audio array with exact original duration (filled with silence)
            if original_duration_samples <= 0:
                logger.warning("No valid duration found, creating minimal audio")
                original_duration_samples = int(sample_rate)  # 1 second fallback
            final_audio = np.zeros(original_duration_samples, dtype=base_dtype)
            # Place cloned audio segments at their exact positions
            for segment in segments:
                cloned_path = segment.get("cloned_audio_path")
                if not cloned_path or not os.path.exists(cloned_path):
                    continue
                    
                try:
                    cloned_audio, _ = sf.read(cloned_path)
                    if len(cloned_audio.shape) > 1:
                        cloned_audio = np.mean(cloned_audio, axis=1)  # Convert to mono
                    
                    start_ms = segment.get("start", 0)
                    end_ms = segment.get("end", 0)
                    
                    # Validate timing values
                    if start_ms < 0 or end_ms <= start_ms:
                        logger.warning(f"Invalid timing for segment {segment.get('id', 'unknown')}: {start_ms}ms-{end_ms}ms")
                        continue
                        
                    start_sample = int((start_ms / 1000.0) * sample_rate)
                    expected_duration_samples = int(((end_ms - start_ms) / 1000.0) * sample_rate)
                    
                    # Ensure cloned audio fits the expected segment duration
                    if len(cloned_audio) > expected_duration_samples:
                        # Truncate if too long
                        cloned_audio = cloned_audio[:expected_duration_samples]
                    elif len(cloned_audio) < expected_duration_samples:
                        # Pad with silence if too short
                        padding = expected_duration_samples - len(cloned_audio)
                        cloned_audio = np.pad(cloned_audio, (0, padding), mode="constant")
                    
                    # Calculate end position
                    end_sample = start_sample + len(cloned_audio)
                    
                    # Only place audio if it fits within original duration
                    if start_sample >= 0 and end_sample <= original_duration_samples:
                        final_audio[start_sample:end_sample] = cloned_audio
                        logger.info(f"Placed segment {segment.get('id', 'unknown')} at {start_ms}ms-{end_ms}ms")
                    else:
                        logger.warning(f"Segment {segment.get('id', 'unknown')} exceeds original audio duration, skipping")
                        
                except Exception as e:
                    logger.error(f"Failed to process segment {segment.get('id', 'unknown')}: {str(e)}")
                    continue
            # Save final audio with exact original duration
            final_path = os.path.join(process_temp_dir, f"final_dubbed_{job_id}.wav").replace('\\', '/')
            sf.write(final_path, final_audio, sample_rate)
            
            duration_seconds = len(final_audio) / sample_rate
            logger.info(f"Final audio reconstructed: {final_path} (duration: {duration_seconds:.2f}s, {len(final_audio)} samples)")
            return final_path
        except Exception as e:
            logger.error(f"Failed to reconstruct final audio: {str(e)}")
            return None


def get_simple_dubbed_api() -> SimpleDubbedAPI:
    """Get or create global SimpleDubbedAPI instance (thread-safe)"""
    global _simple_dubbed_api_instance
    if _simple_dubbed_api_instance is None:
        with _api_lock:
            if _simple_dubbed_api_instance is None:
                _simple_dubbed_api_instance = SimpleDubbedAPI()
    return _simple_dubbed_api_instance
